{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f99bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from DataConversion import ConvertDatasetToBinary_V2, ConvertDatasetToDiscreteK_V2\n",
    "from DataLoader import LoadData_Abalone, LoadData_Banknote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Forest Coverage Dataset\n",
    "from sklearn.datasets import fetch_covtype\n",
    "X, Y = fetch_covtype(return_X_y=True)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris Dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87099c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Abalone Dataset\n",
    "X, Y = LoadData_Abalone()\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39011aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1372, 3)\n",
      "(1372,)\n"
     ]
    }
   ],
   "source": [
    "# Load Banknote Dataset\n",
    "X, Y = LoadData_Banknote()\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a5fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestAccurayOnDataset(X, Y, list_keys, num_trials, test_size_ratio):\n",
    "    \n",
    "    dict_estimator = {}\n",
    "    \n",
    "    for key in list_keys:\n",
    "        dict_key = {'train': [], 'test': []}\n",
    "        dict_estimator[key] = dict_key\n",
    "    \n",
    "    for i_trial in tqdm(range(0, num_trials)):\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size_ratio)\n",
    "        \n",
    "        Classifier_LogReg = LogisticRegression(fit_intercept=True)\n",
    "        clf_LogReg = Classifier_LogReg.fit(X_train, Y_train)\n",
    "        score_LogReg_Train = clf_LogReg.score(X_train, Y_train)\n",
    "        score_LogReg_Test = clf_LogReg.score(X_test, Y_test)\n",
    "        \n",
    "        dict_estimator['LogisticRegression']['train'].append(score_LogReg_Train)\n",
    "        dict_estimator['LogisticRegression']['test'].append(score_LogReg_Test)\n",
    "        \n",
    "        \n",
    "        Classifier_RF = RandomForestClassifier()\n",
    "        clf_RF = Classifier_RF.fit(X_train, Y_train)\n",
    "        score_RF_Train = clf_RF.score(X_train, Y_train)\n",
    "        score_RF_Test = clf_RF.score(X_test, Y_test)\n",
    "        \n",
    "        dict_estimator['RandomForest']['train'].append(score_RF_Train)\n",
    "        dict_estimator['RandomForest']['test'].append(score_RF_Test)\n",
    "        \n",
    "        Classifier_NN = MLPClassifier(hidden_layer_sizes=(5,2,2))\n",
    "        clf_NN = Classifier_NN.fit(X_train, Y_train)\n",
    "        score_NN_Train = clf_NN.score(X_train, Y_train)\n",
    "        score_NN_Test = clf_NN.score(X_test, Y_test)\n",
    "        \n",
    "        dict_estimator['NeuralNetwork']['train'].append(score_NN_Train)\n",
    "        dict_estimator['NeuralNetwork']['test'].append(score_NN_Test)\n",
    "        \n",
    "        Classifier_LinearSVC = LinearSVC()\n",
    "        clf_LinearSVC = Classifier_LinearSVC.fit(X_train, Y_train)\n",
    "        score_LinearSVC_Train = clf_LinearSVC.score(X_train, Y_train)\n",
    "        score_LinearSVC_Test = clf_LinearSVC.score(X_test, Y_test)\n",
    "        \n",
    "        dict_estimator['LinearSVC']['train'].append(score_LinearSVC_Train)\n",
    "        dict_estimator['LinearSVC']['test'].append(score_LinearSVC_Test)\n",
    "        \n",
    "        Classifier_SGD = SGDClassifier()\n",
    "        clf_SGD = Classifier_SGD.fit(X_train, Y_train)\n",
    "        score_SGD_Train = clf_SGD.score(X_train, Y_train)\n",
    "        score_SGD_Test = clf_SGD.score(X_test, Y_test)\n",
    "        \n",
    "        dict_estimator['SGD']['train'].append(score_SGD_Train)\n",
    "        dict_estimator['SGD']['test'].append(score_SGD_Test)\n",
    "        \n",
    "        Classifier_GaussianNB = GaussianNB()\n",
    "        clf_GaussianNB = Classifier_GaussianNB.fit(X_train, Y_train)\n",
    "        score_GaussianNB_Train = clf_GaussianNB.score(X_train, Y_train)\n",
    "        score_GaussianNB_Test = clf_GaussianNB.score(X_test, Y_test)\n",
    "        \n",
    "        dict_estimator['GaussianNB']['train'].append(score_GaussianNB_Train)\n",
    "        dict_estimator['GaussianNB']['test'].append(score_GaussianNB_Test)\n",
    "        \n",
    "        Classifier_LinearRegression = LinearRegression()\n",
    "        clf_LinearRegression = Classifier_LinearRegression.fit(X_train, Y_train)\n",
    "        score_LinearRegression_Train = clf_LinearRegression.score(X_train, Y_train)\n",
    "        score_LinearRegression_Test = clf_LinearRegression.score(X_test, Y_test)\n",
    "        \n",
    "        dict_estimator['LinearRegression']['train'].append(score_LinearRegression_Train)\n",
    "        dict_estimator['LinearRegression']['test'].append(score_LinearRegression_Test)\n",
    "        \n",
    "        Classifier_AdaBoost = AdaBoostClassifier()\n",
    "        clf_AdaBoost = Classifier_AdaBoost.fit(X_train, Y_train)\n",
    "        score_AdaBoost_Train = clf_AdaBoost.score(X_train, Y_train)\n",
    "        score_AdaBoost_Test = clf_AdaBoost.score(X_test, Y_test)\n",
    "        \n",
    "        dict_estimator['AdaBoost']['train'].append(score_AdaBoost_Train)\n",
    "        dict_estimator['AdaBoost']['test'].append(score_AdaBoost_Test)\n",
    "\n",
    "        kernel = 1.0 * RBF(1.0)\n",
    "        Classifier_GP = GaussianProcessClassifier(kernel=kernel,random_state=0)\n",
    "        clf_GP = Classifier_GP.fit(X_train, Y_train)\n",
    "        score_GP_Train = clf_GP.score(X_train, Y_train)\n",
    "        score_GP_Test = clf_GP.score(X_test, Y_test)\n",
    "        \n",
    "        dict_estimator['GaussianProcess']['train'].append(score_GP_Train)\n",
    "        dict_estimator['GaussianProcess']['test'].append(score_GP_Test)\n",
    "        \n",
    "     \n",
    "    return dict_estimator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1124913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EstimateOneDataset(X, Y, list_keys, num_trials, test_size_ratio, list_check_pts):\n",
    "\n",
    "    dict_estimator = TestAccurayOnDataset(X, Y, list_keys, num_trials, test_size_ratio)\n",
    "\n",
    "    dict_train_mean = {var: [] for var in list_keys}\n",
    "    dict_train_std = {var: [] for var in list_keys}\n",
    "\n",
    "    dict_test_mean = {var: [] for var in list_keys}\n",
    "    dict_test_std = {var: [] for var in list_keys}\n",
    "\n",
    "    for key in dict_estimator.keys():\n",
    "\n",
    "        if(key in dict_train_mean):\n",
    "\n",
    "            list_key_est_train_full = dict_estimator[key]['train']\n",
    "            list_key_est_test_full = dict_estimator[key]['test']\n",
    "\n",
    "            for chk_pts in list_check_pts:\n",
    "\n",
    "                list_key_est_train_check_pts = list_key_est_train_full[:chk_pts]\n",
    "                dict_train_mean[key].append(np.mean(list_key_est_train_check_pts))\n",
    "                dict_train_std[key].append(np.std(list_key_est_train_check_pts))\n",
    "\n",
    "                list_key_est_test_check_pts = list_key_est_test_full[:chk_pts]\n",
    "                dict_test_mean[key].append(np.mean(list_key_est_test_check_pts))\n",
    "                dict_test_std[key].append(np.std(list_key_est_test_check_pts))\n",
    "                \n",
    "    return dict_train_mean, dict_train_std, dict_test_mean, dict_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5cec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotResults(list_check_pts, dict_train, dict_test):\n",
    "    \n",
    "    for key in dict_train.keys():\n",
    "        plt.plot(list_check_pts, dict_train[key], '--', label=key+\"Train\")\n",
    "        plt.plot(list_check_pts, dict_test[key], label=key+\"Test\")\n",
    "        \n",
    "        plt.legend(bbox_to_anchor=(1.05, 1.0))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "260090da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotResultsFromDiffDataset(list_check_pts, list_keys, list_dict_name, list_dict_train, list_dict_test):\n",
    "    \n",
    "    for key in list_dict_train[0].keys():\n",
    "        \n",
    "        for i_dict in range(0,len(list_dict_name)):\n",
    "            \n",
    "            str_name_i = list_dict_name[i_dict]\n",
    "            dict_train_i = list_dict_train[i_dict]\n",
    "            dict_test_i = list_dict_test[i_dict]\n",
    "        \n",
    "            plt.plot(list_check_pts, dict_train_i[key], '--', label=key+\"Train\"+str_name_i)\n",
    "            plt.plot(list_check_pts, dict_test_i[key], label=key+\"Test\"+str_name_i)\n",
    "        \n",
    "        plt.legend(bbox_to_anchor=(1.05, 1.0))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec3c527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]\n"
     ]
    }
   ],
   "source": [
    "# Setup Experiment Environment\n",
    "\n",
    "LIST_EST_KEYS = ['LogisticRegression', 'RandomForest','NeuralNetwork','LinearSVC','SGD','GaussianNB','LinearRegression','AdaBoost', 'GaussianProcess']\n",
    "\n",
    "NUM_TRIALS = 200\n",
    "\n",
    "START_TRIAL = 10\n",
    "STEP_TRIAL = 10\n",
    "END_TRIAL_EXCLUSIVE = NUM_TRIALS + STEP_TRIAL\n",
    "\n",
    "LIST_CHECK_PTS = [*range(START_TRIAL, END_TRIAL_EXCLUSIVE, STEP_TRIAL)]\n",
    "print(LIST_CHECK_PTS)\n",
    "\n",
    "TEST_SIZE_RATIO = 0.3333\n",
    "\n",
    "LIST_DICT_NAME = [\"Original\", \"K-Discrete\", \"Binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562579b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_estimator = TestAccurayOnDataset(X, Y, LIST_EST_KEYS, NUM_TRIALS, TEST_SIZE_RATIO)\n",
    "\n",
    "dict_train_mean = {var: [] for var in LIST_EST_KEYS}\n",
    "dict_train_std = {var: [] for var in LIST_EST_KEYS}\n",
    "\n",
    "dict_test_mean = {var: [] for var in LIST_EST_KEYS}\n",
    "dict_test_std = {var: [] for var in LIST_EST_KEYS}\n",
    "\n",
    "for key in dict_estimator.keys():\n",
    "\n",
    "    if(key in dict_train_mean):\n",
    "        \n",
    "        list_key_est_train_full = dict_estimator[key]['train']\n",
    "        list_key_est_test_full = dict_estimator[key]['test']\n",
    "        \n",
    "        for chk_pts in LIST_CHECK_PTS:\n",
    "            \n",
    "            list_key_est_train_check_pts = list_key_est_train_full[:chk_pts]\n",
    "            dict_train_mean[key].append(np.mean(list_key_est_train_check_pts))\n",
    "            dict_train_std[key].append(np.std(list_key_est_train_check_pts))\n",
    "\n",
    "            list_key_est_test_check_pts = list_key_est_test_full[:chk_pts]\n",
    "            dict_test_mean[key].append(np.mean(list_key_est_test_check_pts))\n",
    "            dict_test_std[key].append(np.std(list_key_est_test_check_pts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotResults(LIST_CHECK_PTS, dict_train_mean, dict_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde73d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07fcf50aac141b7812311e6fd513e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_train_mean_ori, dict_train_std_ori, dict_test_mean_ori, dict_test_std_ori = EstimateOneDataset(X, Y, LIST_EST_KEYS, NUM_TRIALS, TEST_SIZE_RATIO, LIST_CHECK_PTS)\n",
    "\n",
    "height_bt = 7\n",
    "\n",
    "type_root = \"half\"\n",
    "\n",
    "X_B, dummy_B = ConvertDatasetToBinary_V2(X, X, height_bt, type_root)\n",
    "\n",
    "dict_train_mean_B, dict_train_std_B, dict_test_mean_B, dict_test_std_B = EstimateOneDataset(X_B, Y, LIST_EST_KEYS, NUM_TRIALS, TEST_SIZE_RATIO, LIST_CHECK_PTS)\n",
    "\n",
    "K_Bins = 10\n",
    "\n",
    "X_K, dummy_K = ConvertDatasetToDiscreteK_V2(X, X, K_Bins)\n",
    "\n",
    "dict_train_mean_K, dict_train_std_K, dict_test_mean_K, dict_test_std_K = EstimateOneDataset(X_K, Y, LIST_EST_KEYS, NUM_TRIALS, TEST_SIZE_RATIO, LIST_CHECK_PTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict_train = [dict_train_mean_ori, dict_train_mean_K, dict_train_mean_B]\n",
    "list_dict_test = [dict_test_mean_ori, dict_test_mean_K, dict_test_mean_B]\n",
    "\n",
    "PlotResultsFromDiffDataset(LIST_CHECK_PTS, LIST_EST_KEYS, LIST_DICT_NAME, list_dict_train, list_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c7ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
